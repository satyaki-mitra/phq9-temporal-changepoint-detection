{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69859534",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f95ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Ignore all the future warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64c0a6",
   "metadata": {},
   "source": [
    "# Sample Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddd75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_patients            = 100\n",
    "total_days                = 365\n",
    "required_sample_count     = 50\n",
    "maximum_surveys_attempted = 7\n",
    "seed                      = 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d629a",
   "metadata": {},
   "source": [
    "# PHQ-9 Score Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcbbcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phq9_samples(time_index):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Specify lower and upper bound of PHQ-9 scores depending on the time_index\n",
    "        if (1 <= time_index <= 5):\n",
    "            upper_bound   = 27\n",
    "            lower_bound   = 10\n",
    "            probabilities = [0.056, 0.056, 0.056, 0.056, 0.028, 0.028, 0.028, 0.028, 0.014, 0.014, 0.014, 0.014, 0.007, 0.007, 0.007, 0.0035, 0.0035, 0.00175]\n",
    "        elif (6 <= time_index <= 10):\n",
    "            upper_bound   = 19\n",
    "            lower_bound   = 10\n",
    "            probabilities = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] \n",
    "        elif (11 <= time_index <= 20):\n",
    "            upper_bound   = 19\n",
    "            lower_bound   = 5\n",
    "            probabilities = [0.06, 0.06, 0.06, 0.03, 0.03, 0.03, 0.015, 0.015, 0.015, 0.0075, 0.0075, 0.0075, 0.00375, 0.00375, 0.00375] \n",
    "        elif (21 <= time_index <= 30):\n",
    "            upper_bound   = 19\n",
    "            lower_bound   = 0\n",
    "            probabilities = [0.05, 0.025, 0.0125, 0.00625, 0.003125, 0.00160625, 0.000803125, 0.0004015625, 0.00020078125, 0.000100390625, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5, 5.01953125e-5]\n",
    "        elif (31 <= time_index <= 40):\n",
    "            upper_bound   = 14\n",
    "            lower_bound   = 0\n",
    "            probabilities = [0.06667, 0.03333, 0.01667, 0.00833, 0.004167, 0.00208, 0.00104, 0.00052, 0.00026, 0.00013, 6.51041e-5, 3.25521e-5, 1.62760e-5, 8.13802e-6, 4.06901e-6]\n",
    "        elif (41 <= time_index <= 100):\n",
    "            upper_bound   = 14\n",
    "            lower_bound   = 0\n",
    "            probabilities = [0.06667, 0.03333, 0.01667, 0.00833, 0.004167, 0.00208, 0.00104, 0.00052, 0.00026, 0.00013, 6.51041e-5, 3.25521e-5, 1.62760e-5, 8.13802e-6, 4.06901e-6]\n",
    "        elif (101 <= time_index <= 220):\n",
    "            upper_bound   = 9\n",
    "            lower_bound   = 0\n",
    "            probabilities = [0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.025]\n",
    "        elif (221 <= time_index <= 300):\n",
    "            upper_bound   = 9\n",
    "            lower_bound   = 0\n",
    "            probabilities = [0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.025]\n",
    "        elif (301 <= time_index <= 365):\n",
    "            upper_bound   = 4\n",
    "            lower_bound   = 0\n",
    "            probabilities = [0.5, 0.25, 0.15, 0.1, 0.05]\n",
    "\n",
    "        # Generating a list of all possible cases of PHQ-9 scores i.e Population within the possible range of variation \n",
    "        population     = list(np.arange(start = lower_bound, \n",
    "                                        stop  = (upper_bound+1),\n",
    "                                        step  = 1,\n",
    "                                        dtype = int))\n",
    "\n",
    "        # Now draw sample of specified size from the above generated population and drawing should be done using Simple\n",
    "        # Random Sampling with replacement as more than one person may have same PHQ-9 score\n",
    "\n",
    "        required_sample = np.random.choice(a       = population,\n",
    "                                           replace = True, \n",
    "                                           size    = 1,\n",
    "                                           p       = [prob/sum(probabilities) for prob in probabilities])\n",
    "\n",
    "        return required_sample \n",
    "    \n",
    "    except Exception as PHQ9GenerationError:\n",
    "        return PHQ9GenerationError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7daecb",
   "metadata": {},
   "source": [
    "# Allotment of patients, survey days and surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72913e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_allocations(total_patients, total_days, required_sample_count, maximum_surveys_attempted):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if (required_sample_count > total_patients):\n",
    "        return repr(ValueError('Total number of patients should be greater than required sample count'))\n",
    "    \n",
    "    try:\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        # Make a list of all possible time stamps\n",
    "        days_list           = list(np.arange(start = 1,\n",
    "                                             stop  = (total_days+1),\n",
    "                                             step  = 1,\n",
    "                                             dtype = int))\n",
    "        \n",
    "        patients_list       = list(np.arange(start = 1,\n",
    "                                             stop  = (total_patients+1),\n",
    "                                             step  = 1,\n",
    "                                             dtype = int))\n",
    "        \n",
    "        # Assign probabilities for each day of getting selected\n",
    "        probabilities       = [0.99]*1 + [0.85]*4 + [0.85]*10 + [0.7]*5 + [0.5]*20 + [0.3]*20 + [0.25]*60 + [0.125]*60 + [0.0625]*60 + [0.03125]*60 + [0.015625]*62 + [0.95]*3\n",
    "        \n",
    "        # Select desired number of samples randomly from the all possible cases\n",
    "        desired_sample_days = sorted(np.random.choice(a       = days_list,\n",
    "                                                      size    = required_sample_count,\n",
    "                                                      replace = False,\n",
    "                                                      p       = [probability / sum(probabilities) for probability in probabilities]))\n",
    "        \n",
    "        # Create an empty dataframe\n",
    "        output_dataframe    = pd.DataFrame(index=[f\"Day_{day}\" for day in desired_sample_days])\n",
    "        \n",
    "        # Generate data for each patient\n",
    "        for day in desired_sample_days:\n",
    "            # Calculate the number of patients attempting surveys for the given day\n",
    "            patients_attempting_survey = int(total_patients * (2 ** (-day / 365)))\n",
    "\n",
    "            # Randomly select the patients attempting surveys\n",
    "            random_patient_attempting  = sorted(random.sample(range(1, total_patients + 1), patients_attempting_survey))\n",
    "            # Generate PHQ-9 scores for each patient\n",
    "            for patient in random_patient_attempting:\n",
    "                # Randomly select the number of surveys attempted by the patient\n",
    "                number_of_surveys_attempted = random.randint(1, maximum_surveys_attempted)\n",
    "\n",
    "                for survey_count in range(number_of_surveys_attempted):\n",
    "                    # Calculate the PHQ-9 score\n",
    "                    score = generate_phq9_samples(time_index=day)\n",
    "                    \n",
    "                    # Skip adding score if survey_count exceeds 6\n",
    "                    if (survey_count >= maximum_surveys_attempted):\n",
    "                        continue\n",
    "                    \n",
    "                    # Add the score to the dataframe\n",
    "                    column_name                                     = f\"Patient_{patient}\"\n",
    "                    output_dataframe.loc[f\"Day_{day}\", column_name] = score[0]\n",
    "        \n",
    "        \n",
    "        # For each patients' select only surveys attempted between a defined range\n",
    "        for column in output_dataframe.columns:\n",
    "            # Get the non-null values in the column\n",
    "            non_nan_values = output_dataframe[column].dropna().values\n",
    "            # Check if there are any non-null values\n",
    "            if (len(non_nan_values) > 0):\n",
    "                # Randomly select a range between 1 and 7\n",
    "                num_cells        = np.random.randint((maximum_surveys_attempted-2), maximum_surveys_attempted)\n",
    "        \n",
    "                # Randomly select indices from non-null values\n",
    "                selected_indices = np.random.choice(a       = len(non_nan_values), \n",
    "                                                    size    = num_cells, \n",
    "                                                    replace = False)\n",
    "        \n",
    "                # Create a mask with True values for the selected indices\n",
    "                mask = np.zeros(len(output_dataframe), dtype=bool)\n",
    "                mask[np.flatnonzero(output_dataframe[column].notna())[selected_indices]] = True\n",
    "        \n",
    "                # Set all other cells in the column as NaN\n",
    "                output_dataframe.loc[~mask, column] = np.nan\n",
    "                \n",
    "        # Set the index name\n",
    "        output_dataframe.index.name = 'Day'\n",
    "        column_order                = [f'Patient_{i}' for i in range(1, (total_patients+1))]\n",
    "        final_output_dataframe      = output_dataframe[column_order]\n",
    "        \n",
    "        return final_output_dataframe\n",
    "    \n",
    "    except Exception as SampleAllocationError:\n",
    "        return repr(f'SampleAllocationError : {SampleAllocationError}').replace('  ', '')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74baca",
   "metadata": {},
   "source": [
    "# Plotting the PHQ-9 Scores' Scatter Plot for different Query Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9aa91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterDiagram_phq9Scores_all_queryDays(input_data : pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not isinstance(input_data, pd.DataFrame)):\n",
    "        return repr(TypeError(f'Expected a pandas Dataframe for the argument : input_data,\\\n",
    "                                got : {type(input_data)} instead')).replace('  ', '')\n",
    "    try:\n",
    "        # Fix the size of the plot\n",
    "        fig, ax   = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "        # Extract and format data\n",
    "        data_dict = input_data.T.to_dict()\n",
    "        keys      = list(data_dict.keys())\n",
    "        values    = list()\n",
    "        for key in keys:\n",
    "            scores = [x for x in list(data_dict[key].values()) if ~np.isnan(x)]\n",
    "            values.append(scores)\n",
    "\n",
    "        # Calculate the cluster sizes based on the number of values in each cluster\n",
    "        circle_sizes = [len(value) for value in values]\n",
    "        \n",
    "        # Plot the PHQ-9 scores for each day along with the color scale based \n",
    "        # on the weightages (Frequencies) of each scores\n",
    "        for key, value, size in zip(keys, values, circle_sizes):\n",
    "            color = np.linspace(start = 0, \n",
    "                                stop  = 1, \n",
    "                                num   = size)\n",
    "            \n",
    "            ax.scatter(x    = [key]*size, \n",
    "                       y    = [element for element in value],\n",
    "                       c    = color,\n",
    "                       cmap = 'viridis',\n",
    "                       s    = 25)\n",
    "\n",
    "        # Set the color scale for PHQ-9 Score Frequency in each day\n",
    "        sm   = plt.cm.ScalarMappable(cmap = 'viridis')\n",
    "        cbar = plt.colorbar(mappable = sm, cax = sm.set_array([]))\n",
    "        cbar.set_label('Weightages')\n",
    "        \n",
    "        # Set the x-axis labels and ticks\n",
    "        plt.xticks(ticks    = range(len(keys)),\n",
    "                   labels   = keys,\n",
    "                   fontsize = 10,\n",
    "                   rotation = 90)\n",
    "        \n",
    "        # Set the y-axis labels and ticks\n",
    "        plt.yticks(ticks    = np.arange(0, 31, 1),\n",
    "                   fontsize = 10,\n",
    "                   rotation = 0)\n",
    "        \n",
    "        # Set Axis Labels and Title of the plot \n",
    "        plt.xlabel(xlabel   = 'Time',\n",
    "                   fontsize = 15)\n",
    "        \n",
    "        plt.ylabel(ylabel   = 'PHQ-9 Scores',\n",
    "                   fontsize = 15)\n",
    "        \n",
    "        plt.title(label    = 'Scatter Plot of PHQ-9 Scores for Given Sample\\n',\n",
    "                  fontsize = 20)\n",
    "        \n",
    "        plt.tight_layout(pad   = 2.0,\n",
    "                         h_pad = 1.0,\n",
    "                         w_pad = 1.0)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as PHQ9ScoresAllPlottingError:\n",
    "        return repr(f'PHQ9ScoresAllPlottingError : {PHQ9ScoresAllPlottingError} while Plotting Scatter diagram\\\n",
    "                      for PHQ-9 scores of all study individuals for all study days').replace('  ', '')\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf65eb",
   "metadata": {},
   "source": [
    "# Plotting Daily Average for all study patients' PHQ-9 Scores for Discrete Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a39475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_average_phq9_line_plot(input_data : pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not isinstance(input_data, pd.DataFrame)):\n",
    "        return repr(TypeError(f'Expected a pandas DataFrame for the argument : input_data, \\\n",
    "                                got : {type(input_data)} instead')).replace('  ', '')\n",
    "    \n",
    "    try:\n",
    "        # Calculate Daily Average PHQ-9 Scores of 100 observed patients\n",
    "        daily_average_scores = input_data.mean(axis   = 1, \n",
    "                                               skipna = True)\n",
    "\n",
    "        # Set the figure size\n",
    "        plt.figure(figsize = (25, 10))\n",
    "        \n",
    "        # Plot the Daily Average PHQ-9 scores by Line Digram\n",
    "        plt.plot(daily_average_scores.index, \n",
    "                 daily_average_scores.values,\n",
    "                 scalex     = True,\n",
    "                 scaley     = True,\n",
    "                 marker     = 'o',\n",
    "                 markersize = 5.0,\n",
    "                 linestyle  = '-',\n",
    "                 linewidth  = 2.0)\n",
    "\n",
    "        plt.xlabel(xlabel   = 'Survey Day (Time)',\n",
    "                   fontsize = 15)\n",
    "\n",
    "        plt.ylabel(ylabel   = 'Average PHQ-9 Score',\n",
    "                   fontsize = 15)\n",
    "\n",
    "        plt.yticks(ticks    = np.arange(0, 31, 1),\n",
    "                   fontsize = 10)\n",
    "\n",
    "        plt.xticks(ticks    = daily_average_scores.index,\n",
    "                   fontsize = 10,\n",
    "                   rotation = 90)\n",
    "\n",
    "        plt.title(label    = 'Trend Line: Average PHQ-9 Score Over 365 Days\\n',\n",
    "                  fontsize = 20)\n",
    "\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout(pad   = 2.0,\n",
    "                         h_pad = 1.0,\n",
    "                         w_pad = 1.0)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as DailyAverageLinePlotError:\n",
    "        return repr(f'DailyAverageLinePlotError : {DailyAverageLinePlotError}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed027458",
   "metadata": {},
   "source": [
    "# Temporal Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57b5e6",
   "metadata": {},
   "source": [
    "## a. Obtain the optimal number of clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc1795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_temporal_clustering(input_data : pd.DataFrame, max_clusters:int) -> None:\n",
    "    \"\"\"\n",
    "    Perform temporal clustering using K-means algorithm and determine the optimal number of clusters\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "        input_data  {DataFrame} : Input data for clustering, where rows represent scores and columns\n",
    "                                  represent days\n",
    "        \n",
    "        max_clusters   {int}    : Maximum number of clusters to consider\n",
    "        \n",
    "        \n",
    "    Errors:\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not isinstance(input_data, pd.DataFrame)):\n",
    "        return repr(TypeError(f'Expected a pandas DataFrame for the argument : input_data,\\\n",
    "                              got : {type(input_data)} instead')).replace('  ', '')\n",
    "    \n",
    "    if (not isinstance(max_clusters, int)):\n",
    "        return repr(TypeError(f'Expected an integer for the argument : max_clusters, got :\\\n",
    "                                {type(max_clusters)} instead')).replace('  ', '')\n",
    "    \n",
    "    try:\n",
    "        # Transpose the input_data to have scores as rows and days as columns\n",
    "        required_dataframe = input_data.T\n",
    "        # Fill the nan values with -1 for computations\n",
    "        required_dataframe = required_dataframe.fillna(value = -1,\n",
    "                                                       axis  = 0)\n",
    "        \n",
    "        # Initialize lists to save results from each iteration\n",
    "        inertia_list      = list()\n",
    "        silhouette_scores = list()\n",
    "        \n",
    "        # Iterate over different number of clusters\n",
    "        for n_clusters in range(2, (max_clusters+2)):\n",
    "            # Instantiate the K-Means model\n",
    "            kmeans           = cluster.KMeans(n_clusters   = n_clusters,\n",
    "                                              random_state = 1234)\n",
    "            # Fit the model on the dataset\n",
    "            kmeans.fit(required_dataframe)\n",
    "            \n",
    "            # Calculate the inertia for the fitted model\n",
    "            inertia          = kmeans.inertia_\n",
    "            \n",
    "            # Calculate the Silhoutte score for checking goodness of fit of the model\n",
    "            silhouette_score = metrics.silhouette_score(X      = required_dataframe,\n",
    "                                                        labels = kmeans.labels_)\n",
    "            # Finally dump the inertia and goodness-of-fit score in their corresponding lists\n",
    "            inertia_list.append(inertia)\n",
    "            silhouette_scores.append(silhouette_score)\n",
    "        \n",
    "        \n",
    "        # Elbow Method - Percentage change in inertia\n",
    "        inertia_percent_change = [100 * (inertia_list[i] - inertia_list[i-1]) / inertia_list[i-1] for i in range(1, len(inertia_list))]\n",
    "        elbow_point            = inertia_percent_change.index(min(inertia_percent_change)) + 2  # Add 2 to account for starting from 2 clusters\n",
    "\n",
    "        # Silhouette Analysis - Optimal number of clusters\n",
    "        optimal_clusters       = silhouette_scores.index(max(silhouette_scores)) + 2  # Add 2 to account for starting from 2 clusters\n",
    "\n",
    "        # Plot the inertia values to identify the elbow point\n",
    "        # Fix the size of the plot\n",
    "        fig, ax   = plt.subplots(figsize=(25, 10))\n",
    "        x_coordinates = np.arange(start = 1,\n",
    "                                  stop  = max_clusters+1,\n",
    "                                  step  = 1,\n",
    "                                  dtype = int).tolist()\n",
    "        y_coordinates = inertia_list\n",
    "\n",
    "        # Draw the Line Plot\n",
    "        plt.plot(x_coordinates,\n",
    "                 y_coordinates,\n",
    "                 scalex     = True,\n",
    "                 scaley     = True,\n",
    "                 marker     = 'o',\n",
    "                 markersize = 5.0,\n",
    "                 linestyle  = '-',\n",
    "                 linewidth  = 2.0)\n",
    "        # Decorators\n",
    "        # Set Ticks and Labels\n",
    "        plt.xticks(ticks    = np.arange(start = (np.min(x_coordinates)-1), \n",
    "                                        stop  = (np.max(x_coordinates)+2),\n",
    "                                        step  = 1,\n",
    "                                        dtype = int),\n",
    "                   fontsize = 10,\n",
    "                   rotation = 0)\n",
    "        \n",
    "        plt.yticks(fontsize = 10,\n",
    "                   rotation = 0)\n",
    "                       \n",
    "        plt.xlabel(xlabel   = 'Number of Clusters',\n",
    "                   fontsize = 12)\n",
    "        \n",
    "        plt.ylabel(ylabel   = 'Inertia',\n",
    "                   fontsize = 12)\n",
    "        \n",
    "        plt.title(label    = 'Optimal Cluster Finding: Elbow Method\\n',\n",
    "                  fontsize = 20)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        # Plot the Silhoutte scores to identify the optimal number of clusters\n",
    "        # Fix the size of the plot\n",
    "        fig, ax   = plt.subplots(figsize=(25, 10))\n",
    "        x_coordinates = np.arange(start = 1,\n",
    "                                  stop  = max_clusters+1,\n",
    "                                  step  = 1,\n",
    "                                  dtype = int).tolist()\n",
    "        y_coordinates = silhouette_scores\n",
    "\n",
    "        # Draw the Line Plot\n",
    "        plt.plot(x_coordinates,\n",
    "                 y_coordinates,\n",
    "                 scalex     = True,\n",
    "                 scaley     = True,\n",
    "                 marker     = 'o',\n",
    "                 markersize = 5.0,\n",
    "                 linestyle  = '-',\n",
    "                 linewidth  = 2.0)\n",
    "        # Decorators\n",
    "        # Set Ticks and Labels\n",
    "        plt.xticks(ticks    = np.arange(start = (np.min(x_coordinates)-1), \n",
    "                                        stop  = (np.max(x_coordinates)+2),\n",
    "                                        step  = 1,\n",
    "                                        dtype = int),\n",
    "                   fontsize = 10,\n",
    "                   rotation = 0)\n",
    "        \n",
    "        plt.yticks(fontsize = 10,\n",
    "                   rotation = 0)\n",
    "                       \n",
    "        plt.xlabel(xlabel   = 'Number of Clusters',\n",
    "                   fontsize = 12)\n",
    "        \n",
    "        plt.ylabel(ylabel   = 'Silhouette Score',\n",
    "                   fontsize = 12)\n",
    "        \n",
    "        plt.title(label    = 'Optimal Cluster Finding: Silhouette Score Method\\n',\n",
    "                  fontsize = 20)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return elbow_point, optimal_clusters\n",
    "    except Exception as KMeansClusteringError:\n",
    "        return repr(f'KMeansClusteringError : Got : {KMeansClusteringError} while performing\\\n",
    "                      Temporal clustering by K-Means method').replace('  ', '')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afcb25",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## b. Fit the Clustering Models with the optimal number of Clusters obtained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdbf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_clustering_model(input_data, optimal_clusters):\n",
    "    \"\"\"\n",
    "    Fit the clustering model with the specified number of clusters and plot the results\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "        input_data {DataFrame} : Input data with days as columns and scores as rows\n",
    "        \n",
    "        n_clusters    {int}    : Number of optimal clusters\n",
    "        \n",
    "    Errors:\n",
    "    -------\n",
    "        TypeError              : Error occurs if any of the input argument is not of its\n",
    "                                 defined data type\n",
    "        \n",
    "        ClusterFittingError    : Error occurs if any exception happens while fitting the \n",
    "                                 Clustering model with the precomputed number of clusters\n",
    "                                 on the input dataset to obtain the cluster labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "             {np.array}        : A numpy array containing the cluster labels for all the \n",
    "                                 input data points\n",
    "    \"\"\"\n",
    "    if (not isinstance(input_data, pd.DataFrame)):\n",
    "        return repr(TypeError(f'Expected a pandas DataFrame for the argument : input_data, got\\\n",
    "                                : {type(input_data)} instead')).replace('  ', '')\n",
    "    \n",
    "    if (not isinstance(optimal_clusters, int)):\n",
    "        return repr(TypeError(f'Expected an integer for the argument : optimal_clusters, got : \\\n",
    "                                {type(optimal_clusters)} instead')).replace('  ', '')\n",
    "    \n",
    "    try:\n",
    "        # Prepare the input data in required Format and then do missing value treatment\n",
    "        required_data  = input_data.T.fillna(value = -1,\n",
    "                                             axis  = 0)\n",
    "        \n",
    "        kmeans         = cluster.KMeans(n_clusters   = optimal_clusters,\n",
    "                                        random_state = 1234)\n",
    "        kmeans.fit(required_data.T)\n",
    "        cluster_labels = kmeans.labels_.tolist()\n",
    "        return cluster_labels\n",
    "    \n",
    "    except Exception as ClusterFittingError:\n",
    "        return repr(f'ClusterFittingError : Got : {ClusterFittingError} while Fitting clusters\\\n",
    "                      on the original dataset').replace('  ', '')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791565f2",
   "metadata": {},
   "source": [
    "## c. Plot the Clusters along with the original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ecd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(input_data: pd.DataFrame, n_clusters: int, cluster_labels: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot the input data with clusters separated by dotted vertical lines\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        input_data      {DataFrame} : Input data with Patients as columns and PHQ-9 scores \n",
    "                                      of different days as rows\n",
    "        \n",
    "        n_clusters         {int}    : Number of optimal clusters\n",
    "        \n",
    "        cluster_labels     {list}   : Cluster labels assigned to each data point\n",
    "        \n",
    "    Errors:\n",
    "    -------\n",
    "        TypeError                   : Error occurs if any of the input argument is not of\n",
    "                                      defined data type\n",
    "                                      \n",
    "        ClusterPlottingError        : Error occurs if any exception occurs while plotting\n",
    "                                      the clusters on the original dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "                  {None}            : A None type object\n",
    "    \"\"\"\n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        return repr(TypeError(f'Expected a pandas DataFrame for the argument: input_data, got: \\\n",
    "                                {type(input_data)} instead')).replace('  ', '')\n",
    "    \n",
    "    if not isinstance(n_clusters, int):\n",
    "        return repr(TypeError(f'Expected an integer for the argument: n_clusters, got: \\\n",
    "                                {type(n_clusters)} instead')).replace('  ', '')\n",
    "    \n",
    "    if not isinstance(cluster_labels, list):\n",
    "        return repr(TypeError(f'Expected a Python list for the argument: cluster_labels, got: \\\n",
    "                                {type(cluster_labels)} instead')).replace('  ', '')\n",
    "    \n",
    "    try:\n",
    "        required_data = input_data.T\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot PHQ-9 scores over days with color scale for score frequencies\n",
    "        for i in range(required_data.shape[0]):\n",
    "            scores = required_data.iloc[i]\n",
    "            color_scale = scores.value_counts(normalize=True)\n",
    "            colors = [color_scale.get(score, 0) for score in scores]\n",
    "            plt.scatter(required_data.columns, scores, c=colors, cmap='hot', marker='o', label=f'Score {i+1}')\n",
    "        \n",
    "        # Plot vertical lines to separate clusters\n",
    "        cluster_boundaries = np.where(cluster_labels[:-1] != cluster_labels[1:])[0]\n",
    "        unique_labels = np.unique(cluster_labels)\n",
    "        colors = plt.cm.nipy_spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "        for boundary in cluster_boundaries:\n",
    "            cluster_label = cluster_labels[boundary]\n",
    "            color = colors[np.where(unique_labels == cluster_label)[0][0]]\n",
    "            plt.axvline(x=boundary, linestyle='dotted', color=color)\n",
    "\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('PHQ-9 Score')\n",
    "        plt.title(f'Clustering Results: {n_clusters} Clusters')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.colorbar(label='Score Frequency')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as ClusterPlottingError:\n",
    "        return repr(f'ClusterPlottingError: Got {ClusterPlottingError} while plotting clusters \\\n",
    "                      on the original dataset').replace('  ', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed1467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa37f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce9dccf",
   "metadata": {},
   "source": [
    "# Drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3d03d",
   "metadata": {},
   "source": [
    "### Driver for generating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25cb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sample_allocations(total_patients            = total_patients,\n",
    "                                 total_days                = total_days,\n",
    "                                 required_sample_count     = required_sample_count,\n",
    "                                 maximum_surveys_attempted = maximum_surveys_attempted)\n",
    "\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.to_csv('../data/PHQ_9_sample_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172d53b",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics = sample_data.T.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d6173",
   "metadata": {},
   "source": [
    "### Scatter Diagram : Day to Day Distribution of PHQ-9 Scores with Patient level granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample raw data on a scatter diagram\n",
    "scatterDiagram_phq9Scores_all_queryDays(input_data = sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1635d2",
   "metadata": {},
   "source": [
    "### Trend Line :  Daily Average of PHQ-9 Sample over a time-period of 365 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88906aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_average_phq9_line_plot(input_data = sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346f221",
   "metadata": {},
   "source": [
    "### Temporal Clustering : Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad069c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_point, optimal_clusters = kmeans_temporal_clustering(input_data   = sample_data,\n",
    "                                                           max_clusters = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Elbow Point         : {elbow_point}')\n",
    "print (f'Silhouette Clusters : {optimal_clusters}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-Means model with Silhouette Optimal Number of Clusters\n",
    "cluster_labels = fit_clustering_model(input_data       = sample_data,\n",
    "                                      optimal_clusters = optimal_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528bf30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Clusters\n",
    "plot_clusters(input_data     = sample_data,\n",
    "              n_clusters     = optimal_clusters,\n",
    "              cluster_labels = cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b846f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eae353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a1c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab9c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def fit_clustering_model_1(input_data, min_samples=2):\n",
    "    \"\"\"\n",
    "    Fit the clustering model with consideration of temporal continuity and plot the results\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "        input_data      {DataFrame} : Input data with days as columns and scores as rows\n",
    "        min_samples     {int}       : Minimum number of samples required for clustering (default: 2)\n",
    "        \n",
    "    Errors:\n",
    "    -------\n",
    "        TypeError                   : Error occurs if any of the input argument is not of defined data type\n",
    "        ClusterFittingError         : Error occurs if any exception happens while fitting the clustering model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        cluster_labels   {np.array} : A numpy array containing the cluster labels for all the input data points\n",
    "    \"\"\"\n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        return repr(TypeError(f\"Expected a pandas DataFrame for the argument: input_data, got: {type(input_data)} instead\")).replace('  ', '')\n",
    "\n",
    "    try:\n",
    "        # Sort the input data by dates\n",
    "        input_data = input_data.sort_index(axis=1)\n",
    "        \n",
    "        # Impute missing values with the mean\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        input_data_imputed = pd.DataFrame(imputer.fit_transform(input_data), columns=input_data.columns, index=input_data.index)\n",
    "        \n",
    "        # Normalize the input data\n",
    "        scaler = MinMaxScaler()\n",
    "        input_data_normalized = pd.DataFrame(scaler.fit_transform(input_data_imputed), columns=input_data_imputed.columns, index=input_data_imputed.index)\n",
    "        \n",
    "        # Calculate pairwise distances between consecutive days using KL divergence\n",
    "        pairwise_dists = np.zeros((input_data_normalized.shape[1]-1, input_data_normalized.shape[1]-1))\n",
    "        for i in range(input_data_normalized.shape[1]-1):\n",
    "            for j in range(i+1, input_data_normalized.shape[1]):\n",
    "                dist = entropy(input_data_normalized.iloc[:, i], input_data_normalized.iloc[:, j])\n",
    "                pairwise_dists[i, j-1] = dist\n",
    "                pairwise_dists[j-1, i] = dist\n",
    "\n",
    "        # Fit OPTICS clustering model\n",
    "        optics = OPTICS(min_samples=min_samples, metric=\"precomputed\")\n",
    "        optics.fit(pairwise_dists)\n",
    "\n",
    "        # Obtain the cluster labels\n",
    "        cluster_labels = optics.labels_\n",
    "\n",
    "        # Plot the scatter plot of PHQ-9 scores with cluster boundaries\n",
    "        plot_clusters_with_boundaries(input_data_normalized, cluster_labels)\n",
    "\n",
    "        return cluster_labels\n",
    "\n",
    "    except Exception as ClusterFittingError:\n",
    "        return repr(f\"ClusterFittingError: {ClusterFittingError} while fitting clusters on the original dataset\").replace('  ', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_miss_data = sample_data.copy().fillna(-1)\n",
    "fit_clustering_model_1(input_data  = no_miss_data, \n",
    "                       min_samples = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93100259",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd33d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509203be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c48e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdbc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac2218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7b6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66364440",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [23, 24, 24, 21, 20, 22, 21, 24, 21, 23, 24, 24, 23, 24, 25, 24, 23, 25, 23, 25, 22, 22, 23, 26, 25, 24, 25, 22, 21, 21, 25, 26, 22, 22, 22, 22, 23, 21, 23, 24, 21, 23, 22, 22, 22, 26, 21, 23, 21, 22, 22, 21, 25, 24, 23, 26, 21, 27, 22, 22, 25, 20, 25, 23, 23, 21, 25, 23, 23, 23, 27, 24, 22, 24, 26, 24, 24, 20, 19, 27, 23, 22, 21, 24, 21, 21, 24, 23, 25, 22, 22, 23, 21, 25, 24, 23, 24, 21, 23, 25, 24, 24, 21, 23, 25, 23, 23, 24, 23, 26, 23, 19, 24, 24, 21, 22, 20, 24, 23, 23]\n",
    "\n",
    "sns.distplot(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_2 = [22, 25, 20, 18, 19, 21, 17, 16, 20, 17, 19, 20, 20, 19, 22, 21, 17, 19, 19, 21, 23, 20, 20, 20, 18, 19, 19, 19, 21, 19, 22, 20, 18, 24, 22, 21, 22, 22, 19, 19, 16, 19, 19, 19, 19, 19, 17, 20, 16, 20, 24, 17, 19, 20, 19, 19, 17, 22, 21, 17, 16, 21, 22, 17, 23, 21, 19, 20, 22, 22, 19, 21, 21, 18, 17, 21, 19, 19, 22, 23, 20, 21, 20, 18, 21, 21, 16, 17, 17, 19, 18, 21, 19, 25, 22, 23, 18, 19, 21, 22, 18, 19, 18, 18, 20, 18, 21, 20, 19, 19, 21, 22, 19, 23, 20, 20, 18, 19, 23, 18]\n",
    "\n",
    "sns.distplot(values_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d61cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def generateStepsCountData(total_minutes, desired_sum, desired_range, percentage_zeros):\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    # Calculate the number of zero values and non-zero values\n",
    "    num_zeros         = int(total_minutes*percentage_zeros)\n",
    "    \n",
    "    num_non_zeros     = (total_minutes - num_zeros)\n",
    "    \n",
    "    # Generate random non-zero values that mimic activity patterns\n",
    "    non_zero_values   = list()\n",
    "    for i in range(num_non_zeros):\n",
    "        tmp_sum        = 0\n",
    "        non_zero_value = random.choice(np.arange(desired_range[0], desired_range[1], 1))\n",
    "        tmp_sum        = sum(non_zero_values)+non_zero_value\n",
    "        \n",
    "        if (tmp_sum < desired_sum):\n",
    "            non_zero_values.append(non_zero_value)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    remaining_non_zero_values = (num_non_zeros - len(non_zero_values))\n",
    "    if (remaining_non_zero_values > 0):\n",
    "        final_non_zero_values = non_zero_values + [0] * remaining_non_zero_values\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Generate zero values\n",
    "    zero_values       = np.zeros(shape = num_zeros,\n",
    "                                 dtype = int)\n",
    "    \n",
    "    # Combine zero and non-zero values\n",
    "    sample            = np.concatenate((non_zero_values, zero_values),\n",
    "                                       axis   = 0)\n",
    "    \n",
    "    # Shuffle the sample \n",
    "    np.random.shuffle(sample)\n",
    "        \n",
    "    final_sample = sample.copy()\n",
    "    \n",
    "    sum_sample   = sum(final_sample)\n",
    "    while (sum_sample < desired_sum):\n",
    "        for item in range(len(final_sample) - 1, -1, -1):\n",
    "            if (final_sample[item] == 0):\n",
    "                final_sample[item] = (desired_sum - sum_sample)\n",
    "                break\n",
    "            \n",
    "        \n",
    "        \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_minutes    = 24 * 60 # 24 hours * 60 minutes\n",
    "desired_sum      = 3500\n",
    "desired_range    = (1, 150)\n",
    "percentage_zeros = 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count_data = generateStepsCountData(total_minutes    = total_minutes,\n",
    "                                         desired_sum      = desired_sum,\n",
    "                                         desired_range    = desired_range,\n",
    "                                         percentage_zeros = percentage_zeros) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c66064d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step_count_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m(\u001b[43mstep_count_data\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'step_count_data' is not defined"
     ]
    }
   ],
   "source": [
    "sum(step_count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3d025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
